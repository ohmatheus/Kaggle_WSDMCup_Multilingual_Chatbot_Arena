{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T10:58:15.406868Z",
     "iopub.status.busy": "2024-12-31T10:58:15.406529Z",
     "iopub.status.idle": "2024-12-31T10:58:18.880251Z",
     "shell.execute_reply": "2024-12-31T10:58:18.879320Z",
     "shell.execute_reply.started": "2024-12-31T10:58:15.406840Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers: 4.47.1\n"
     ]
    }
   ],
   "source": [
    "# gemma-2 is available from transformers>=4.42.3\n",
    "\n",
    "import transformers as trsf\n",
    "print(\"Transformers:\", trsf.__version__)\n",
    "\n",
    "#!pip install -U \"transformers>=4.42.3\" bitsandbytes accelerate peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T10:58:18.881621Z",
     "iopub.status.busy": "2024-12-31T10:58:18.881321Z",
     "iopub.status.idle": "2024-12-31T10:58:29.909550Z",
     "shell.execute_reply": "2024-12-31T10:58:29.908664Z",
     "shell.execute_reply.started": "2024-12-31T10:58:18.881601Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "from dataclasses import dataclass\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "#from datasets import Dataset\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import (\n",
    "    BitsAndBytesConfig,\n",
    "    Gemma2ForSequenceClassification,\n",
    "    Gemma2Model,\n",
    "    GemmaTokenizerFast,\n",
    "    Gemma2Config,\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    AutoModelForSequenceClassification,\n",
    "    PreTrainedTokenizerBase, \n",
    "    EvalPrediction,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    "    get_cosine_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import ModelsUtils as Utils\n",
    "import Configurations as Configs\n",
    "#import wsdm_modelutils as Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peft: 0.14.0\n"
     ]
    }
   ],
   "source": [
    "import peft as pft\n",
    "print(\"Peft:\", pft.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T10:58:29.911929Z",
     "iopub.status.busy": "2024-12-31T10:58:29.911151Z",
     "iopub.status.idle": "2024-12-31T10:58:29.966701Z",
     "shell.execute_reply": "2024-12-31T10:58:29.965793Z",
     "shell.execute_reply.started": "2024-12-31T10:58:29.911901Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.5.1+cu118\n",
      "Torch is build with CUDA: True\n",
      "Torch device : cuda\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('Torch version:', torch.__version__)\n",
    "print('Torch is build with CUDA:', torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Torch device : {device}')\n",
    "print('------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'micro_gemma2_2b_fp16_4bit'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_file = 'Configs.py'\n",
    "manager = Configs.ConfigManager(config_file)\n",
    "\n",
    "config = manager.micro\n",
    "#config = manager.runpod_1\n",
    "#load_from_config = manager.save_load_gemma2_2b_fp16\n",
    "#load_from_config = manager.save_load_gemma2_2b_fp16_hidden_512\n",
    "config.config_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_path = config.basemodel_path\n",
    "\n",
    "peft_model_path = '../Checkpoints/'\n",
    "checkpoint_name = \"Original_notrain\"\n",
    "dataframe_path = config.train_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T10:58:30.092963Z",
     "iopub.status.busy": "2024-12-31T10:58:30.092618Z",
     "iopub.status.idle": "2024-12-31T10:58:30.097699Z",
     "shell.execute_reply": "2024-12-31T10:58:30.096874Z",
     "shell.execute_reply.started": "2024-12-31T10:58:30.092930Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=config.lora_r,\n",
    "    lora_alpha=config.lora_alpha,\n",
    "    # only target self-attention\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n",
    "    #layers_to_transform=[i for i in range(42) if i >= config.freeze_layers],\n",
    "    lora_dropout=config.lora_dropout,\n",
    "    bias=config.lora_bias,\n",
    "    task_type=TaskType.SEQ_CLS\n",
    ")\n",
    "\n",
    "# lora_config = LoraConfig(\n",
    "#     r=config.lora_r,\n",
    "#     lora_alpha=config.lora_alpha,\n",
    "#     # only target self-attention\n",
    "#     target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n",
    "#     #target_modules=[\"all-linear\"],\n",
    "#     #target_modules=[\"self_attn\"],\n",
    "#     layers_to_transform=[i for i in range(config.max_layers) if i >= config.freeze_layers],\n",
    "#     #layers_to_transform=[0],\n",
    "#     layers_pattern=\"layers\",\n",
    "#     lora_dropout=config.lora_dropout,\n",
    "#     bias=config.lora_bias,\n",
    "#     #task_type=TaskType.FEATURE_EXTRACTION, #SEQ_CLS\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T10:58:30.098881Z",
     "iopub.status.busy": "2024-12-31T10:58:30.098561Z",
     "iopub.status.idle": "2024-12-31T10:58:30.205889Z",
     "shell.execute_reply": "2024-12-31T10:58:30.204902Z",
     "shell.execute_reply.started": "2024-12-31T10:58:30.098850Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ohmatheus\\AppData\\Local\\Temp\\ipykernel_24528\\552262335.py:1: DtypeWarning: Columns (0,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(dataframe_path)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "      <th>winner</th>\n",
       "      <th>language</th>\n",
       "      <th>class_label</th>\n",
       "      <th>prompt_len</th>\n",
       "      <th>response_a_len</th>\n",
       "      <th>response_b_len</th>\n",
       "      <th>...</th>\n",
       "      <th>prompt_chinese</th>\n",
       "      <th>prompt_round_balance</th>\n",
       "      <th>prompt_curly_balance</th>\n",
       "      <th>prompt_json</th>\n",
       "      <th>prompt_sentiment</th>\n",
       "      <th>response_a_sentiment</th>\n",
       "      <th>response_b_sentiment</th>\n",
       "      <th>cosine_similarity_a</th>\n",
       "      <th>cosine_similarity_b</th>\n",
       "      <th>cosine_similarity_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>53567</td>\n",
       "      <td>What is the difference between marriage licens...</td>\n",
       "      <td>A marriage license is a legal document that al...</td>\n",
       "      <td>A marriage license and a marriage certificate ...</td>\n",
       "      <td>model_b</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>192</td>\n",
       "      <td>3096</td>\n",
       "      <td>3592</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.077778</td>\n",
       "      <td>0.058469</td>\n",
       "      <td>0.139458</td>\n",
       "      <td>0.705321</td>\n",
       "      <td>0.629803</td>\n",
       "      <td>0.075518</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                             prompt  \\\n",
       "0  53567  What is the difference between marriage licens...   \n",
       "\n",
       "                                          response_a  \\\n",
       "0  A marriage license is a legal document that al...   \n",
       "\n",
       "                                          response_b   winner language  \\\n",
       "0  A marriage license and a marriage certificate ...  model_b      NaN   \n",
       "\n",
       "   class_label  prompt_len  response_a_len  response_b_len  ...  \\\n",
       "0            1         192            3096            3592  ...   \n",
       "\n",
       "   prompt_chinese  prompt_round_balance  prompt_curly_balance  prompt_json  \\\n",
       "0             0.0                     0                     0            0   \n",
       "\n",
       "   prompt_sentiment  response_a_sentiment  response_b_sentiment  \\\n",
       "0          0.077778              0.058469              0.139458   \n",
       "\n",
       "   cosine_similarity_a  cosine_similarity_b  cosine_similarity_diff  \n",
       "0             0.705321             0.629803                0.075518  \n",
       "\n",
       "[1 rows x 70 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(dataframe_path)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T10:58:30.212194Z",
     "iopub.status.busy": "2024-12-31T10:58:30.211905Z",
     "iopub.status.idle": "2024-12-31T10:58:30.227854Z",
     "shell.execute_reply": "2024-12-31T10:58:30.227016Z",
     "shell.execute_reply.started": "2024-12-31T10:58:30.212162Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df['prompt'] = df['prompt'].astype(str)\n",
    "df['response_a'] = df['response_a'].astype(str)\n",
    "df['response_b'] = df['response_b'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T10:58:30.228835Z",
     "iopub.status.busy": "2024-12-31T10:58:30.228544Z",
     "iopub.status.idle": "2024-12-31T10:58:31.987413Z",
     "shell.execute_reply": "2024-12-31T10:58:31.986421Z",
     "shell.execute_reply.started": "2024-12-31T10:58:30.228809Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "tokenizer.add_eos_token = True      # We'll add <eos> at the end\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=config.sample_size, random_state=config.random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(175, 70)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_df(row):\n",
    "    return Utils.tokenize(tokenizer, [row['prompt']], [row['response_a']], [row['response_b']], max_length=config.max_length)\n",
    "\n",
    "df['tokens'] = df.apply(tokenize_df, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "      <th>winner</th>\n",
       "      <th>language</th>\n",
       "      <th>class_label</th>\n",
       "      <th>prompt_len</th>\n",
       "      <th>response_a_len</th>\n",
       "      <th>response_b_len</th>\n",
       "      <th>...</th>\n",
       "      <th>prompt_round_balance</th>\n",
       "      <th>prompt_curly_balance</th>\n",
       "      <th>prompt_json</th>\n",
       "      <th>prompt_sentiment</th>\n",
       "      <th>response_a_sentiment</th>\n",
       "      <th>response_b_sentiment</th>\n",
       "      <th>cosine_similarity_a</th>\n",
       "      <th>cosine_similarity_b</th>\n",
       "      <th>cosine_similarity_diff</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>52573</th>\n",
       "      <td>469a23f177362d53b509d6c3242f35da8d152d2d0773e1...</td>\n",
       "      <td>根据下面的描述生成一个病历：\\n\\n扁鹊见蔡桓公，立有间，扁鹊曰：“君有疾在腠理，不治将恐深...</td>\n",
       "      <td>**患者姓名**: 蔡桓公  \\n**性别**: 男  \\n**年龄**: 不详  \\n**...</td>\n",
       "      <td>## 蔡桓公病历\\n\\n**患者姓名:** 蔡桓公\\n\\n**性别:** 男\\n\\n**年龄...</td>\n",
       "      <td>model_b</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>1</td>\n",
       "      <td>289</td>\n",
       "      <td>543</td>\n",
       "      <td>711</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.122169</td>\n",
       "      <td>-0.122169</td>\n",
       "      <td>{'input_ids': [[tensor(2), tensor(235322), ten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6383</th>\n",
       "      <td>695419302</td>\n",
       "      <td>Please exmplain what are the various form of t...</td>\n",
       "      <td>Transfer learning is a technique in machine le...</td>\n",
       "      <td>Transfer learning is a machine learning techni...</td>\n",
       "      <td>model_a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>116</td>\n",
       "      <td>2439</td>\n",
       "      <td>2637</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.123044</td>\n",
       "      <td>0.086212</td>\n",
       "      <td>0.709314</td>\n",
       "      <td>0.726687</td>\n",
       "      <td>-0.017373</td>\n",
       "      <td>{'input_ids': [[tensor(2), tensor(235322), ten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37366</th>\n",
       "      <td>4093550403</td>\n",
       "      <td>I am a physically challenged girl and an  inte...</td>\n",
       "      <td>As a physically challenged and internally disp...</td>\n",
       "      <td>Title: Unleashing Potential Through Entreprene...</td>\n",
       "      <td>model_b</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1122</td>\n",
       "      <td>5164</td>\n",
       "      <td>5814</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.069118</td>\n",
       "      <td>0.074009</td>\n",
       "      <td>0.053008</td>\n",
       "      <td>0.867659</td>\n",
       "      <td>0.838861</td>\n",
       "      <td>0.028798</td>\n",
       "      <td>{'input_ids': [[tensor(2), tensor(235322), ten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20324</th>\n",
       "      <td>2206386265</td>\n",
       "      <td>Pretend you are a girl that is reacting to you...</td>\n",
       "      <td>Mother: \"Dear, we need to talk about your outf...</td>\n",
       "      <td>Mother: \"Sweetie, I've picked out the perfect ...</td>\n",
       "      <td>model_b</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>1028</td>\n",
       "      <td>939</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.800000</td>\n",
       "      <td>0.241898</td>\n",
       "      <td>0.187642</td>\n",
       "      <td>0.731589</td>\n",
       "      <td>0.733099</td>\n",
       "      <td>-0.001510</td>\n",
       "      <td>{'input_ids': [[tensor(2), tensor(235322), ten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57177</th>\n",
       "      <td>5ed610ae2199a489f0f03453d394061308e5fb48bacac1...</td>\n",
       "      <td>Какие автотрассировщики есть для Altium 24?</td>\n",
       "      <td>Для Altium Designer 24, как и для предыдущих в...</td>\n",
       "      <td>В Altium Designer 24 для автоматической прокла...</td>\n",
       "      <td>model_a</td>\n",
       "      <td>Russian</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>1762</td>\n",
       "      <td>1317</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.606370</td>\n",
       "      <td>0.792594</td>\n",
       "      <td>-0.186224</td>\n",
       "      <td>{'input_ids': [[tensor(2), tensor(235322), ten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86315</th>\n",
       "      <td>f9215e484f2f9520bf0060ecdf0d677699f721681a90bf...</td>\n",
       "      <td>Help me rephrase the paragraph below: \\n\\n\"\"\"\\...</td>\n",
       "      <td>Here's a rephrased version of the paragraph:\\n...</td>\n",
       "      <td>Certainly! Here's a rephrased version of the p...</td>\n",
       "      <td>model_a</td>\n",
       "      <td>English</td>\n",
       "      <td>0</td>\n",
       "      <td>1714</td>\n",
       "      <td>1268</td>\n",
       "      <td>1745</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.198052</td>\n",
       "      <td>0.053571</td>\n",
       "      <td>0.104345</td>\n",
       "      <td>0.700518</td>\n",
       "      <td>0.924211</td>\n",
       "      <td>-0.223693</td>\n",
       "      <td>{'input_ids': [[tensor(2), tensor(235322), ten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38667</th>\n",
       "      <td>4234203318</td>\n",
       "      <td>What does it mean when a girl looks at you str...</td>\n",
       "      <td>Ah, my dear anime fan, as a guru anime girl lo...</td>\n",
       "      <td>Oh ho ho! It seems you've caught the eye of a ...</td>\n",
       "      <td>model_a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>319</td>\n",
       "      <td>4909</td>\n",
       "      <td>2109</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.067500</td>\n",
       "      <td>0.086618</td>\n",
       "      <td>0.293948</td>\n",
       "      <td>0.748433</td>\n",
       "      <td>0.537206</td>\n",
       "      <td>0.211227</td>\n",
       "      <td>{'input_ids': [[tensor(2), tensor(235322), ten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42446</th>\n",
       "      <td>10e2c4f5dedc8ab49f090b7de2c5996782f3233f757637...</td>\n",
       "      <td>What is tres leche</td>\n",
       "      <td>Tres Leches Cake, or \"Pastel de Tres Leches\" i...</td>\n",
       "      <td>Tres leches is a Latin American dessert that t...</td>\n",
       "      <td>model_b</td>\n",
       "      <td>English</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>2867</td>\n",
       "      <td>949</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.178636</td>\n",
       "      <td>0.219444</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.643726</td>\n",
       "      <td>0.063380</td>\n",
       "      <td>{'input_ids': [[tensor(2), tensor(235322), ten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22992</th>\n",
       "      <td>2493251791</td>\n",
       "      <td>Teach me about probability</td>\n",
       "      <td>Probability is a branch of mathematics that de...</td>\n",
       "      <td>Probability is a branch of mathematics that de...</td>\n",
       "      <td>model_b</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>1471</td>\n",
       "      <td>2474</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100794</td>\n",
       "      <td>0.105864</td>\n",
       "      <td>0.570214</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.070214</td>\n",
       "      <td>{'input_ids': [[tensor(2), tensor(235322), ten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56450</th>\n",
       "      <td>5b3b9e366a6b7074bddf25b1f116135cc52132c1a303fb...</td>\n",
       "      <td>AWS의 멀티 어카운트에 대해서 각각의 account의 리소스 정보 및 CUR 데이...</td>\n",
       "      <td>## AWS Multi-Account LLM Chatbot: Resource Con...</td>\n",
       "      <td>AWS의 멀티 어카운트 환경에서 각각의 어카운트의 리소스 정보와 Cost and U...</td>\n",
       "      <td>model_a</td>\n",
       "      <td>Korean</td>\n",
       "      <td>0</td>\n",
       "      <td>136</td>\n",
       "      <td>611</td>\n",
       "      <td>277</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.084184</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.176777</td>\n",
       "      <td>0.502079</td>\n",
       "      <td>-0.325302</td>\n",
       "      <td>{'input_ids': [[tensor(2), tensor(235322), ten...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>175 rows × 71 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      id  \\\n",
       "52573  469a23f177362d53b509d6c3242f35da8d152d2d0773e1...   \n",
       "6383                                           695419302   \n",
       "37366                                         4093550403   \n",
       "20324                                         2206386265   \n",
       "57177  5ed610ae2199a489f0f03453d394061308e5fb48bacac1...   \n",
       "...                                                  ...   \n",
       "86315  f9215e484f2f9520bf0060ecdf0d677699f721681a90bf...   \n",
       "38667                                         4234203318   \n",
       "42446  10e2c4f5dedc8ab49f090b7de2c5996782f3233f757637...   \n",
       "22992                                         2493251791   \n",
       "56450  5b3b9e366a6b7074bddf25b1f116135cc52132c1a303fb...   \n",
       "\n",
       "                                                  prompt  \\\n",
       "52573  根据下面的描述生成一个病历：\\n\\n扁鹊见蔡桓公，立有间，扁鹊曰：“君有疾在腠理，不治将恐深...   \n",
       "6383   Please exmplain what are the various form of t...   \n",
       "37366  I am a physically challenged girl and an  inte...   \n",
       "20324  Pretend you are a girl that is reacting to you...   \n",
       "57177        Какие автотрассировщики есть для Altium 24?   \n",
       "...                                                  ...   \n",
       "86315  Help me rephrase the paragraph below: \\n\\n\"\"\"\\...   \n",
       "38667  What does it mean when a girl looks at you str...   \n",
       "42446                                 What is tres leche   \n",
       "22992                         Teach me about probability   \n",
       "56450  AWS의 멀티 어카운트에 대해서 각각의 account의 리소스 정보 및 CUR 데이...   \n",
       "\n",
       "                                              response_a  \\\n",
       "52573  **患者姓名**: 蔡桓公  \\n**性别**: 男  \\n**年龄**: 不详  \\n**...   \n",
       "6383   Transfer learning is a technique in machine le...   \n",
       "37366  As a physically challenged and internally disp...   \n",
       "20324  Mother: \"Dear, we need to talk about your outf...   \n",
       "57177  Для Altium Designer 24, как и для предыдущих в...   \n",
       "...                                                  ...   \n",
       "86315  Here's a rephrased version of the paragraph:\\n...   \n",
       "38667  Ah, my dear anime fan, as a guru anime girl lo...   \n",
       "42446  Tres Leches Cake, or \"Pastel de Tres Leches\" i...   \n",
       "22992  Probability is a branch of mathematics that de...   \n",
       "56450  ## AWS Multi-Account LLM Chatbot: Resource Con...   \n",
       "\n",
       "                                              response_b   winner language  \\\n",
       "52573  ## 蔡桓公病历\\n\\n**患者姓名:** 蔡桓公\\n\\n**性别:** 男\\n\\n**年龄...  model_b  Chinese   \n",
       "6383   Transfer learning is a machine learning techni...  model_a      NaN   \n",
       "37366  Title: Unleashing Potential Through Entreprene...  model_b      NaN   \n",
       "20324  Mother: \"Sweetie, I've picked out the perfect ...  model_b      NaN   \n",
       "57177  В Altium Designer 24 для автоматической прокла...  model_a  Russian   \n",
       "...                                                  ...      ...      ...   \n",
       "86315  Certainly! Here's a rephrased version of the p...  model_a  English   \n",
       "38667  Oh ho ho! It seems you've caught the eye of a ...  model_a      NaN   \n",
       "42446  Tres leches is a Latin American dessert that t...  model_b  English   \n",
       "22992  Probability is a branch of mathematics that de...  model_b      NaN   \n",
       "56450  AWS의 멀티 어카운트 환경에서 각각의 어카운트의 리소스 정보와 Cost and U...  model_a   Korean   \n",
       "\n",
       "       class_label  prompt_len  response_a_len  response_b_len  ...  \\\n",
       "52573            1         289             543             711  ...   \n",
       "6383             0         116            2439            2637  ...   \n",
       "37366            1        1122            5164            5814  ...   \n",
       "20324            1         200            1028             939  ...   \n",
       "57177            0          43            1762            1317  ...   \n",
       "...            ...         ...             ...             ...  ...   \n",
       "86315            0        1714            1268            1745  ...   \n",
       "38667            0         319            4909            2109  ...   \n",
       "42446            1          18            2867             949  ...   \n",
       "22992            1          26            1471            2474  ...   \n",
       "56450            0         136             611             277  ...   \n",
       "\n",
       "       prompt_round_balance  prompt_curly_balance  prompt_json  \\\n",
       "52573                     0                     0            0   \n",
       "6383                      0                     0            0   \n",
       "37366                     0                     0            0   \n",
       "20324                     0                     0            0   \n",
       "57177                     0                     0            0   \n",
       "...                     ...                   ...          ...   \n",
       "86315                     0                     0            0   \n",
       "38667                     0                     0            0   \n",
       "42446                     0                     0            0   \n",
       "22992                     0                     0            0   \n",
       "56450                     0                     0            0   \n",
       "\n",
       "       prompt_sentiment  response_a_sentiment  response_b_sentiment  \\\n",
       "52573          0.000000              0.000000              0.000000   \n",
       "6383           0.000000              0.123044              0.086212   \n",
       "37366         -0.069118              0.074009              0.053008   \n",
       "20324         -0.800000              0.241898              0.187642   \n",
       "57177          0.000000              0.066667              0.000000   \n",
       "...                 ...                   ...                   ...   \n",
       "86315          0.198052              0.053571              0.104345   \n",
       "38667          0.067500              0.086618              0.293948   \n",
       "42446          0.000000              0.178636              0.219444   \n",
       "22992          0.000000              0.100794              0.105864   \n",
       "56450          0.000000              0.084184              0.214286   \n",
       "\n",
       "       cosine_similarity_a  cosine_similarity_b  cosine_similarity_diff  \\\n",
       "52573             0.000000             0.122169               -0.122169   \n",
       "6383              0.709314             0.726687               -0.017373   \n",
       "37366             0.867659             0.838861                0.028798   \n",
       "20324             0.731589             0.733099               -0.001510   \n",
       "57177             0.606370             0.792594               -0.186224   \n",
       "...                    ...                  ...                     ...   \n",
       "86315             0.700518             0.924211               -0.223693   \n",
       "38667             0.748433             0.537206                0.211227   \n",
       "42446             0.707107             0.643726                0.063380   \n",
       "22992             0.570214             0.500000                0.070214   \n",
       "56450             0.176777             0.502079               -0.325302   \n",
       "\n",
       "                                                  tokens  \n",
       "52573  {'input_ids': [[tensor(2), tensor(235322), ten...  \n",
       "6383   {'input_ids': [[tensor(2), tensor(235322), ten...  \n",
       "37366  {'input_ids': [[tensor(2), tensor(235322), ten...  \n",
       "20324  {'input_ids': [[tensor(2), tensor(235322), ten...  \n",
       "57177  {'input_ids': [[tensor(2), tensor(235322), ten...  \n",
       "...                                                  ...  \n",
       "86315  {'input_ids': [[tensor(2), tensor(235322), ten...  \n",
       "38667  {'input_ids': [[tensor(2), tensor(235322), ten...  \n",
       "42446  {'input_ids': [[tensor(2), tensor(235322), ten...  \n",
       "22992  {'input_ids': [[tensor(2), tensor(235322), ten...  \n",
       "56450  {'input_ids': [[tensor(2), tensor(235322), ten...  \n",
       "\n",
       "[175 rows x 71 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T10:58:31.988746Z",
     "iopub.status.busy": "2024-12-31T10:58:31.988410Z",
     "iopub.status.idle": "2024-12-31T10:58:32.000618Z",
     "shell.execute_reply": "2024-12-31T10:58:31.999668Z",
     "shell.execute_reply.started": "2024-12-31T10:58:31.988714Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_train, df_valid = train_test_split(df, test_size=0.1, random_state=config.random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#-------------------------------------------------------------------\n",
    "class ChatbotArenaDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, test=False, max_length=256):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.num_classes = 2\n",
    "        self.test = test\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "\n",
    "        # Tokenize the text\n",
    "        #tokens = Utils.tokenize(self.tokenizer, [row['prompt']], [row['response_a']], [row['response_b']], max_length=self.max_length)\n",
    "        tokens = row['tokens']\n",
    "        \n",
    "        if not self.test:\n",
    "            # Label\n",
    "            label = torch.nn.functional.one_hot(torch.tensor(row['class_label']), num_classes=self.num_classes).float()\n",
    "            #label = torch.tensor([row['class_label']]).float()\n",
    "\n",
    "            return {\n",
    "                'input_ids': tokens['input_ids'].squeeze(0),\n",
    "                'attention_mask': tokens['attention_mask'].squeeze(0),\n",
    "                'label': label\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'input_ids': tokens['input_ids'].squeeze(0),\n",
    "                'attention_mask': tokens['attention_mask'].squeeze(0),\n",
    "            }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T10:58:32.001612Z",
     "iopub.status.busy": "2024-12-31T10:58:32.001364Z",
     "iopub.status.idle": "2024-12-31T10:58:32.016352Z",
     "shell.execute_reply": "2024-12-31T10:58:32.015349Z",
     "shell.execute_reply.started": "2024-12-31T10:58:32.001593Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Prepare dataset and dataloader\n",
    "dataset_train = ChatbotArenaDataset(df_train, tokenizer, max_length=config.max_length)\n",
    "dataloader_train = Utils.DataLoader(dataset_train, batch_size=config.train_batch, shuffle=True)\n",
    "\n",
    "dataset_valid = ChatbotArenaDataset(df_valid, tokenizer, max_length=config.max_length)\n",
    "dataloader_valid = Utils.DataLoader(dataset_valid, batch_size=config.eval_batch, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Gemma2ForSequenceClassification were not initialized from the model checkpoint at unsloth/gemma-2-2b and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "quantization_config=BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    #bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    #bnb_4bit_quant_type='nf4',\n",
    "    #bnb_4bit_use_double_quant=False,\n",
    "    )\n",
    "\n",
    "model_base = AutoModelForSequenceClassification.from_pretrained(config.transformers_basemodel_path, \n",
    "            #torch_dtype=torch.float16,\n",
    "            device_map=device, \n",
    "            quantization_config=quantization_config,\n",
    "            )\n",
    "\n",
    "model_base.config.use_cache = False\n",
    "model_base = prepare_model_for_kbit_training(model_base)\n",
    "lora_model = get_peft_model(model_base, lora_config)\n",
    "\n",
    "# predictionModel = Utils.custom_load_model_chkpt(\n",
    "#                         config,\n",
    "#                         checkpointName=\"Original_notrain\",\n",
    "#                         device=device\n",
    "#                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,583,936 || all params: 2,618,930,432 || trainable%: 0.1750\n"
     ]
    }
   ],
   "source": [
    "lora_model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "execution_failed": "2024-12-31T10:59:52.168Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Gemma2ForSequenceClassification(\n",
       "      (model): Gemma2Model(\n",
       "        (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-25): 26 x Gemma2DecoderLayer(\n",
       "            (self_attn): Gemma2Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2304, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2304, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2304, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2304, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2304, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2304, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear4bit(in_features=2048, out_features=2304, bias=False)\n",
       "              (rotary_emb): Gemma2RotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Gemma2MLP(\n",
       "              (gate_proj): Linear4bit(in_features=2304, out_features=9216, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=2304, out_features=9216, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=9216, out_features=2304, bias=False)\n",
       "              (act_fn): PytorchGELUTanh()\n",
       "            )\n",
       "            (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "            (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "            (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "            (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "      )\n",
       "      (score): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=2304, out_features=2, bias=False)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=2304, out_features=2, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#-------------------------------------------------------------------\n",
    "# Evaluation (used for trainning)\n",
    "def evaluate_model(model, dataloader, device=\"cuda\"):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, total=len(dataloader), unit='row'):\n",
    "\n",
    "            logits = model(\n",
    "                input_ids=batch['input_ids'].to(device),\n",
    "                attention_mask=batch['attention_mask'].to(device),\n",
    "            ).logits\n",
    "\n",
    "            labels = batch['label'].to(device)  # One-hot encoded labels\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_fn(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Compute predictions and accuracy\n",
    "            normLogits = (logits>0.5).float()\n",
    "            predictions = torch.argmax(logits, dim=1)  # Class with highest score\n",
    "            true_labels = torch.argmax(labels, dim=1)  # Convert one-hot to class indices\n",
    "            \n",
    "            correct += (predictions == true_labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    # Calculate average loss and accuracy\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct / total_samples\n",
    "\n",
    "    return {\n",
    "        'loss': avg_loss,\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------\n",
    "def train_model(model, dataloader, valid_dataloader, optimizer, config, scheduler=None,  device=\"cuda\"):\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    min_val_loss = float('inf') #checkpoint\n",
    "    min_acc = 0\n",
    "    history = {\"train_accum_loss\" : [], \"train_accum_accuracy\" : [], \"valid_loss\" : [], \n",
    "                \"valid_accuracy\" : []}\n",
    "    history[\"best_epoch\"]=0\n",
    "    history[\"best_loss\"]=0\n",
    "    history[\"best_acc\"]=0\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(config.n_epochs):\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        correct = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        for batch in tqdm(dataloader, total=len(dataloader), unit='row'):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            logits = model(\n",
    "                input_ids=batch['input_ids'].to(device),\n",
    "                attention_mask=batch['attention_mask'].to(device),\n",
    "            ).logits\n",
    "            \n",
    "            # One-hot labels\n",
    "            labels = batch['label'].to(device)\n",
    "        \n",
    "            loss = loss_fn(logits, labels)\n",
    "        \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Compute predictions and accuracy\n",
    "            normLogits = (logits>0.5).float()\n",
    "            predictions = torch.argmax(logits, dim=1)  # Class with highest score\n",
    "            true_labels = torch.argmax(labels, dim=1)  # Convert one-hot to class indices\n",
    "            \n",
    "            correct += (predictions == true_labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "        \n",
    "        # add date and hour + epochs in checkpoint_name\n",
    "        # Calculate average loss and accuracy\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        accuracy = correct / total_samples\n",
    "\n",
    "        metrics = evaluate_model(model, valid_dataloader, device=device)\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1} Finished\")\n",
    "        print(f\"Accumulated Train Loss: {avg_loss}\")\n",
    "        print(f\"Accumulated Train Accuracy: {accuracy}\")\n",
    "        print(f\"Valid Loss: {metrics['loss']}, Valid Accuracy : {metrics['accuracy']}\")\n",
    "        \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(lora_model.parameters(), weight_decay=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "execution_failed": "2024-12-31T10:59:52.168Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/79 [00:00<?, ?row/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Projects\\Kaggle\\2_WSDMCup_Multilingual_Chatbot_Arena\\.venv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      " 33%|███▎      | 26/79 [01:28<02:59,  3.40s/row]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlora_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[21], line 83\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, dataloader, valid_dataloader, optimizer, config, scheduler, device)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m scheduler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     81\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 83\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# Compute predictions and accuracy\u001b[39;00m\n\u001b[0;32m     86\u001b[0m normLogits \u001b[38;5;241m=\u001b[39m (logits\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = train_model(lora_model, dataloader_train, dataloader_valid, optimizer, config, scheduler=None, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'config_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot_model_history\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTrainning History\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\Projects\\Kaggle\\2_WSDMCup_Multilingual_Chatbot_Arena\\Code\\ModelsUtils.py:136\u001b[0m, in \u001b[0;36mplot_model_history\u001b[1;34m(history, title)\u001b[0m\n\u001b[0;32m    134\u001b[0m acc \u001b[38;5;241m=\u001b[39m history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_accum_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    135\u001b[0m val_acc \u001b[38;5;241m=\u001b[39m history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m--> 136\u001b[0m config_name \u001b[38;5;241m=\u001b[39m \u001b[43mhistory\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mconfig_name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;66;03m# show best epoch with a red line ?\u001b[39;00m\n\u001b[0;32m    140\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(loss) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'config_name'"
     ]
    }
   ],
   "source": [
    "Utils.plot_model_history(history, \"Trainning History\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!runpodctl remove pod $RUNPOD_POD_ID"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6401106,
     "sourceId": 10337400,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6401199,
     "sourceId": 10337556,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 215481246,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 205269,
     "modelInstanceId": 183069,
     "sourceId": 214733,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 205295,
     "modelInstanceId": 183093,
     "sourceId": 214762,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 205701,
     "modelInstanceId": 183509,
     "sourceId": 215248,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
