{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T10:58:15.406868Z",
     "iopub.status.busy": "2024-12-31T10:58:15.406529Z",
     "iopub.status.idle": "2024-12-31T10:58:18.880251Z",
     "shell.execute_reply": "2024-12-31T10:58:18.879320Z",
     "shell.execute_reply.started": "2024-12-31T10:58:15.406840Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers: 4.47.1\n"
     ]
    }
   ],
   "source": [
    "# gemma-2 is available from transformers>=4.42.3\n",
    "\n",
    "import transformers as trsf\n",
    "print(\"Transformers:\", trsf.__version__)\n",
    "\n",
    "#!pip install -U \"transformers>=4.42.3\" bitsandbytes accelerate peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T10:58:18.881621Z",
     "iopub.status.busy": "2024-12-31T10:58:18.881321Z",
     "iopub.status.idle": "2024-12-31T10:58:29.909550Z",
     "shell.execute_reply": "2024-12-31T10:58:29.908664Z",
     "shell.execute_reply.started": "2024-12-31T10:58:18.881601Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "from dataclasses import dataclass\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "#from datasets import Dataset\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import (\n",
    "    BitsAndBytesConfig,\n",
    "    Gemma2ForSequenceClassification,\n",
    "    Gemma2Model,\n",
    "    GemmaTokenizerFast,\n",
    "    Gemma2Config,\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    PreTrainedTokenizerBase, \n",
    "    EvalPrediction,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import ModelsUtils as Utils\n",
    "import Configurations as Configs\n",
    "#import wsdm_modelutils as Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peft: 0.14.0\n"
     ]
    }
   ],
   "source": [
    "import peft as pft\n",
    "print(\"Peft:\", pft.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T10:58:29.911929Z",
     "iopub.status.busy": "2024-12-31T10:58:29.911151Z",
     "iopub.status.idle": "2024-12-31T10:58:29.966701Z",
     "shell.execute_reply": "2024-12-31T10:58:29.965793Z",
     "shell.execute_reply.started": "2024-12-31T10:58:29.911901Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.5.1+cu118\n",
      "Torch is build with CUDA: True\n",
      "Torch device : cuda\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('Torch version:', torch.__version__)\n",
    "print('Torch is build with CUDA:', torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Torch device : {device}')\n",
    "print('------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'micro_gemma2_2b_fp16_4bit'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_file = 'Configs.py'\n",
    "manager = Configs.ConfigManager(config_file)\n",
    "\n",
    "config = manager.micro_test\n",
    "#config = manager.runpod_1\n",
    "#load_from_config = manager.save_load_gemma2_2b_fp16\n",
    "#load_from_config = manager.save_load_gemma2_2b_fp16_hidden_512\n",
    "config.config_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for AutoModel and auto tokenizer\n",
    "#base_model_path = '/kaggle/input/bge-multilingual-gemma2-fp16/pytorch/default/1/bge-multilingual-gemma2-fp16'\n",
    "#base_model_path = '/kaggle/input/bge-multilingual-gemma2-fp16/pytorch/default/1/bge-multilingual-gemma2-fp16-4bit'\n",
    "base_model_path = config.basemodel_path\n",
    "\n",
    "\n",
    "# peft pre saved locally\n",
    "#peft_model_path = '/kaggle/input/peftchkpt_original/pytorch/default/1/'\n",
    "peft_model_path = '../Checkpoints/'\n",
    "checkpoint_name = \"Original_notrain\"\n",
    "\n",
    "#df = pd.read_csv('/kaggle/input/wsdm-preprocessed-full-original/train_preprocessed_FULL_original.csv')\n",
    "#dataframe_path = '/kaggle/input/train-preprocessed-mini-original/train_preprocessed_MINI_original.csv'\n",
    "dataframe_path = config.train_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainning Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T10:58:30.031396Z",
     "iopub.status.busy": "2024-12-31T10:58:30.031115Z",
     "iopub.status.idle": "2024-12-31T10:58:30.090850Z",
     "shell.execute_reply": "2024-12-31T10:58:30.089972Z",
     "shell.execute_reply.started": "2024-12-31T10:58:30.031376Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#training_args = TrainingArguments(\n",
    "#    output_dir=\"output\",\n",
    "#    overwrite_output_dir=True,\n",
    "#    report_to=\"none\",\n",
    "#    num_train_epochs=config.n_epochs,\n",
    "#    per_device_train_batch_size=config.train_batch,\n",
    "#    gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "#    per_device_eval_batch_size=config.eval_batch,\n",
    "#    logging_steps=10,\n",
    "#    eval_strategy=\"epoch\",\n",
    "#    save_strategy=\"steps\",\n",
    "#    save_steps=200,\n",
    "#    optim=config.optim_type,\n",
    "#    fp16=True,\n",
    "#    learning_rate=config.lr,\n",
    "#    warmup_steps=config.warmup_steps,\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T10:58:30.092963Z",
     "iopub.status.busy": "2024-12-31T10:58:30.092618Z",
     "iopub.status.idle": "2024-12-31T10:58:30.097699Z",
     "shell.execute_reply": "2024-12-31T10:58:30.096874Z",
     "shell.execute_reply.started": "2024-12-31T10:58:30.092930Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=config.lora_r,\n",
    "    lora_alpha=config.lora_alpha,\n",
    "    # only target self-attention\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n",
    "    #layers_to_transform=[i for i in range(42) if i >= config.freeze_layers],\n",
    "    lora_dropout=config.lora_dropout,\n",
    "    bias=config.lora_bias,\n",
    "    task_type=TaskType.FEATURE_EXTRACTION, #SEQ_CLS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T10:58:30.098881Z",
     "iopub.status.busy": "2024-12-31T10:58:30.098561Z",
     "iopub.status.idle": "2024-12-31T10:58:30.205889Z",
     "shell.execute_reply": "2024-12-31T10:58:30.204902Z",
     "shell.execute_reply.started": "2024-12-31T10:58:30.098850Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ohmatheus\\AppData\\Local\\Temp\\ipykernel_23356\\552262335.py:1: DtypeWarning: Columns (0,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(dataframe_path)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "      <th>winner</th>\n",
       "      <th>language</th>\n",
       "      <th>class_label</th>\n",
       "      <th>prompt_len</th>\n",
       "      <th>response_a_len</th>\n",
       "      <th>response_b_len</th>\n",
       "      <th>...</th>\n",
       "      <th>prompt_chinese</th>\n",
       "      <th>prompt_round_balance</th>\n",
       "      <th>prompt_curly_balance</th>\n",
       "      <th>prompt_json</th>\n",
       "      <th>prompt_sentiment</th>\n",
       "      <th>response_a_sentiment</th>\n",
       "      <th>response_b_sentiment</th>\n",
       "      <th>cosine_similarity_a</th>\n",
       "      <th>cosine_similarity_b</th>\n",
       "      <th>cosine_similarity_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>53567</td>\n",
       "      <td>What is the difference between marriage licens...</td>\n",
       "      <td>A marriage license is a legal document that al...</td>\n",
       "      <td>A marriage license and a marriage certificate ...</td>\n",
       "      <td>model_b</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>192</td>\n",
       "      <td>3096</td>\n",
       "      <td>3592</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.077778</td>\n",
       "      <td>0.058469</td>\n",
       "      <td>0.139458</td>\n",
       "      <td>0.705321</td>\n",
       "      <td>0.629803</td>\n",
       "      <td>0.075518</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                             prompt  \\\n",
       "0  53567  What is the difference between marriage licens...   \n",
       "\n",
       "                                          response_a  \\\n",
       "0  A marriage license is a legal document that al...   \n",
       "\n",
       "                                          response_b   winner language  \\\n",
       "0  A marriage license and a marriage certificate ...  model_b      NaN   \n",
       "\n",
       "   class_label  prompt_len  response_a_len  response_b_len  ...  \\\n",
       "0            1         192            3096            3592  ...   \n",
       "\n",
       "   prompt_chinese  prompt_round_balance  prompt_curly_balance  prompt_json  \\\n",
       "0             0.0                     0                     0            0   \n",
       "\n",
       "   prompt_sentiment  response_a_sentiment  response_b_sentiment  \\\n",
       "0          0.077778              0.058469              0.139458   \n",
       "\n",
       "   cosine_similarity_a  cosine_similarity_b  cosine_similarity_diff  \n",
       "0             0.705321             0.629803                0.075518  \n",
       "\n",
       "[1 rows x 70 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(dataframe_path)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T10:58:30.212194Z",
     "iopub.status.busy": "2024-12-31T10:58:30.211905Z",
     "iopub.status.idle": "2024-12-31T10:58:30.227854Z",
     "shell.execute_reply": "2024-12-31T10:58:30.227016Z",
     "shell.execute_reply.started": "2024-12-31T10:58:30.212162Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df['prompt'] = df['prompt'].astype(str)\n",
    "df['response_a'] = df['response_a'].astype(str)\n",
    "df['response_b'] = df['response_b'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T10:58:30.228835Z",
     "iopub.status.busy": "2024-12-31T10:58:30.228544Z",
     "iopub.status.idle": "2024-12-31T10:58:31.987413Z",
     "shell.execute_reply": "2024-12-31T10:58:31.986421Z",
     "shell.execute_reply.started": "2024-12-31T10:58:30.228809Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "tokenizer.add_eos_token = True      # We'll add <eos> at the end\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=config.sample_size, random_state=config.random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8765, 70)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T10:58:31.988746Z",
     "iopub.status.busy": "2024-12-31T10:58:31.988410Z",
     "iopub.status.idle": "2024-12-31T10:58:32.000618Z",
     "shell.execute_reply": "2024-12-31T10:58:31.999668Z",
     "shell.execute_reply.started": "2024-12-31T10:58:31.988714Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_train, df_valid = train_test_split(df, test_size=0.1, random_state=config.random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T10:58:32.001612Z",
     "iopub.status.busy": "2024-12-31T10:58:32.001364Z",
     "iopub.status.idle": "2024-12-31T10:58:32.016352Z",
     "shell.execute_reply": "2024-12-31T10:58:32.015349Z",
     "shell.execute_reply.started": "2024-12-31T10:58:32.001593Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Prepare dataset and dataloader\n",
    "dataset_train = Utils.ChatbotArenaDataset(df_train, tokenizer, max_length=config.max_length)\n",
    "dataloader_train = Utils.DataLoader(dataset_train, batch_size=config.train_batch, shuffle=True)\n",
    "\n",
    "dataset_valid = Utils.ChatbotArenaDataset(df_valid, tokenizer, max_length=config.max_length)\n",
    "dataloader_valid = Utils.DataLoader(dataset_valid, batch_size=config.eval_batch, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3944"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataloader_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([     2, 235322,  39038,  78880,  59795,    674,  28161,    576,   1370,\n",
       "            552,    108,   4158,   5968,    603, 235248, 235274, 235276,    578,\n",
       "          28161,    576,   1370,    519,   1693,   4158,   5968,    603, 235248,\n",
       "         235274, 235318,    108, 235265,  48100,    519, 235340, 235254,    109,\n",
       "         235322,   4250, 235298, 235250,  78880,    714,  28161,    576,    573,\n",
       "           1370,    552,   4158,   5968,    603,   2764,    731,    573,  10513,\n",
       "         235292,    109, 130984,    589,    591, 235274,  19734, 235274, 235284,\n",
       "           6596,  18049, 235254, 235393, 235284,    728, 235248, 235274, 235275,\n",
       "            109,   1734,    708,   2764,    674,    573,  28161,    603, 235248,\n",
       "         235274, 235276, 235269,    712,    783,    798,   1142,    908,    573,\n",
       "           2412,   8738, 235292,    109, 235274, 235276,    589,    591, 235274,\n",
       "          19734, 235274, 235284,   6596,  18049, 235254, 235393, 235284,    728,\n",
       "         235248, 235274, 235275,    109,  92261,    574,   2145,  10012,    731,\n",
       "         235248, 235274, 235284, 235269,    783,    947, 235292,    109, 235274,\n",
       "         235284, 235276,    589,    552, 235393, 235284,    728, 235248, 235274,\n",
       "            109,  44718, 235248, 235274,    577,   2145,  10012, 235269,    783,\n",
       "            947, 235292,    109, 235254, 235393, 235284,    589, 235248, 235274,\n",
       "         235284, 235274,    109,  40128,    573,   7800,   6880,    576,   2145,\n",
       "          10012, 235269,    783,    947, 235292,    109, 235254,    589, 235248,\n",
       "         235274, 235274,    591,  14131,    552,  12786,    573,   1758,    576,\n",
       "           4158,   5968, 235269,    665,   2004,    614,   6222, 235275,    109,\n",
       "          54130, 235269,    573,  28161,    576,    573,   1370,    519,   1693,\n",
       "           4158,   5968,    603,   2764,    731,    573,  10513, 235292,    109,\n",
       "         130984,    589,    591, 235274,  19734, 235304,   6596,  18049, 235262,\n",
       "         235393, 235284,    728, 235248, 235274, 235275,    109,   1734,    708,\n",
       "           2764,    674,    573,  28161,    603, 235248, 235274, 235318, 235269,\n",
       "            712,    783,    798,   1142,    908,    573,   2412,   8738, 235292,\n",
       "            109, 235274, 235318,    589,    591, 235274,  19734, 235304,   6596,\n",
       "          18049, 235262, 235393,      1]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " 'features': tensor([ 1.1900e+02,  9.6200e+02,  4.6900e+02,  2.3000e+01,  2.1800e+02,\n",
       "          1.0600e+02,  1.0000e+00,  1.4000e+01,  4.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  4.0000e+00,  3.0000e+00,  1.0000e+00,  2.2000e+01,\n",
       "          3.4000e+01,  0.0000e+00,  0.0000e+00,  8.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  8.0000e+00,  0.0000e+00,  1.0000e+01,  9.0000e+00,\n",
       "          0.0000e+00,  1.0000e+01,  9.0000e+00,  2.5000e+01,  3.0900e+02,\n",
       "          2.3600e+02,  4.0000e+00,  5.7000e+01,  6.3000e+01,  7.3950e-01,\n",
       "          6.0499e-01,  3.4968e-01,  1.6807e-02,  1.4553e-02,  1.2793e-02,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  1.7500e-01, -1.0284e-01,  1.7500e-01,\n",
       "          9.2266e-01,  9.0321e-01,  1.9445e-02]),\n",
       " 'label': tensor([0.])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "f:\\Projects\\Kaggle\\2_WSDMCup_Multilingual_Chatbot_Arena\\.venv\\Lib\\site-packages\\transformers\\quantizers\\auto.py:186: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n"
     ]
    }
   ],
   "source": [
    "predictionModel = Utils.custom_load_model_chkpt(\n",
    "                        config,\n",
    "                        checkpointName=\"Original_notrain\",\n",
    "                        device=device\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "execution_failed": "2024-12-31T10:59:52.168Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreferencePredictionModel(\n",
       "  (gemma_model): PeftModelForFeatureExtraction(\n",
       "    (base_model): LoraModel(\n",
       "      (model): Gemma2Model(\n",
       "        (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-25): 26 x Gemma2DecoderLayer(\n",
       "            (self_attn): Gemma2Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2304, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2304, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2304, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2304, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2304, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2304, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear4bit(in_features=2048, out_features=2304, bias=False)\n",
       "              (rotary_emb): Gemma2RotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Gemma2MLP(\n",
       "              (gate_proj): Linear4bit(in_features=2304, out_features=9216, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=2304, out_features=9216, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=9216, out_features=2304, bias=False)\n",
       "              (act_fn): PytorchGELUTanh()\n",
       "            )\n",
       "            (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "            (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "            (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "            (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (feature_fc): Linear(in_features=63, out_features=128, bias=True)\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=2432, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Linear(in_features=512, out_features=1, bias=True)\n",
       "    (4): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "execution_failed": "2024-12-31T10:59:52.168Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW([\n",
    "    {'params': predictionModel.gemma_model.parameters(), 'lr': 2e-6},     # Lower learning rate for transformer layers\n",
    "    {'params': predictionModel.classifier.parameters(), 'lr': 1e-3},      # Higher learning rate for custom layers\n",
    "], weight_decay=0.01)\n",
    "\n",
    "#optimizer = optim.AdamW(weight_decay=0.01)\n",
    "#optimizer = optim.Adam(predictionModel.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dataloader_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.2000e+01, 4.8000e+01, 3.0000e+00, 2.0000e+00, 2.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 3.0000e+00, 8.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 5.0000e-01, 6.8750e-01, 0.0000e+00,\n",
       "         8.3333e-02, 4.1667e-02, 0.0000e+00, 8.3333e-02, 4.1667e-02, 3.3333e-01,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 4.1667e-01, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [1.3300e+02, 3.7400e+02, 4.4400e+02, 2.9000e+01, 7.5000e+01, 8.8000e+01,\n",
       "         1.0000e+00, 7.0000e+00, 9.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 2.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 5.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 3.2000e+01, 1.0000e+02, 1.2100e+02,\n",
       "         5.0000e+00, 1.3000e+01, 1.1000e+01, 7.1429e-01, 6.7914e-01, 6.8243e-01,\n",
       "         7.5188e-03, 1.8717e-02, 2.0270e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 3.7500e-01, 4.7500e-01, 3.3492e-01,\n",
       "         5.7740e-01, 5.5106e-01, 2.6345e-02]], device='cuda:0')"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats = batch['features'].to(device)\n",
    "feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [1.]], device='cuda:0')"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = batch['label'].to(device)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1.], device='cuda:0')"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionModel.to(device)\n",
    "predictionModel.eval()\n",
    "with torch.no_grad():\n",
    "    logits = predictionModel(\n",
    "                    input_ids=batch['input_ids'].to(device),\n",
    "                    attention_mask=batch['attention_mask'].to(device),\n",
    "                    features=batch['features'].to(device)\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5476],\n",
       "        [0.3642]], device='cuda:0')"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5476, 0.3642], device='cuda:0')"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [0.]], device='cuda:0')"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normLogits = (logits>0.5).float()\n",
    "normLogits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = normLogits\n",
    "true_labels = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [0.]], device='cuda:0')"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [1.]], device='cuda:0')"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(predictions == true_labels).sum().item() / labels.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8061, device='cuda:0')"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = nn.BCELoss()(logits, labels)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2304"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictionModel.gemma_model.config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[147], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mstop\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2024-12-31T10:59:52.168Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "history = Utils.train_model(predictionModel, dataloader_train, dataloader_valid, optimizer, config, scheduler=None, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Utils.plot_model_history(history, \"Trainning History\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!runpodctl remove pod $RUNPOD_POD_ID"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6401106,
     "sourceId": 10337400,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6401199,
     "sourceId": 10337556,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 215481246,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 205269,
     "modelInstanceId": 183069,
     "sourceId": 214733,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 205295,
     "modelInstanceId": 183093,
     "sourceId": 214762,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 205701,
     "modelInstanceId": 183509,
     "sourceId": 215248,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
