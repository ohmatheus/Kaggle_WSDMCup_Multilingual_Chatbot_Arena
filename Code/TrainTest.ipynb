{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T10:58:15.406868Z",
     "iopub.status.busy": "2024-12-31T10:58:15.406529Z",
     "iopub.status.idle": "2024-12-31T10:58:18.880251Z",
     "shell.execute_reply": "2024-12-31T10:58:18.879320Z",
     "shell.execute_reply.started": "2024-12-31T10:58:15.406840Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers: 4.47.1\n"
     ]
    }
   ],
   "source": [
    "# gemma-2 is available from transformers>=4.42.3\n",
    "\n",
    "import transformers as trsf\n",
    "print(\"Transformers:\", trsf.__version__)\n",
    "\n",
    "#!pip install -U \"transformers>=4.42.3\" bitsandbytes accelerate peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T10:58:18.881621Z",
     "iopub.status.busy": "2024-12-31T10:58:18.881321Z",
     "iopub.status.idle": "2024-12-31T10:58:29.909550Z",
     "shell.execute_reply": "2024-12-31T10:58:29.908664Z",
     "shell.execute_reply.started": "2024-12-31T10:58:18.881601Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "from dataclasses import dataclass\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "#from datasets import Dataset\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import (\n",
    "    BitsAndBytesConfig,\n",
    "    Gemma2ForSequenceClassification,\n",
    "    Gemma2Model,\n",
    "    GemmaTokenizerFast,\n",
    "    Gemma2Config,\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    PreTrainedTokenizerBase, \n",
    "    EvalPrediction,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import ModelsUtils as Utils\n",
    "import Configurations as Configs\n",
    "#import wsdm_modelutils as Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peft: 0.14.0\n"
     ]
    }
   ],
   "source": [
    "import peft as pft\n",
    "print(\"Peft:\", pft.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T10:58:29.911929Z",
     "iopub.status.busy": "2024-12-31T10:58:29.911151Z",
     "iopub.status.idle": "2024-12-31T10:58:29.966701Z",
     "shell.execute_reply": "2024-12-31T10:58:29.965793Z",
     "shell.execute_reply.started": "2024-12-31T10:58:29.911901Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.5.1+cu118\n",
      "Torch is build with CUDA: True\n",
      "Torch device : cuda\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('Torch version:', torch.__version__)\n",
    "print('Torch is build with CUDA:', torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Torch device : {device}')\n",
    "print('------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'micro_gemma2_2b_fp16_4bit'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_file = 'Configs.py'\n",
    "manager = Configs.ConfigManager(config_file)\n",
    "\n",
    "config = manager.micro_test\n",
    "#config = manager.runpod_1\n",
    "#load_from_config = manager.save_load_gemma2_2b_fp16\n",
    "#load_from_config = manager.save_load_gemma2_2b_fp16_hidden_512\n",
    "config.config_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for AutoModel and auto tokenizer\n",
    "#base_model_path = '/kaggle/input/bge-multilingual-gemma2-fp16/pytorch/default/1/bge-multilingual-gemma2-fp16'\n",
    "#base_model_path = '/kaggle/input/bge-multilingual-gemma2-fp16/pytorch/default/1/bge-multilingual-gemma2-fp16-4bit'\n",
    "base_model_path = config.basemodel_path\n",
    "\n",
    "\n",
    "# peft pre saved locally\n",
    "#peft_model_path = '/kaggle/input/peftchkpt_original/pytorch/default/1/'\n",
    "peft_model_path = '../Checkpoints/'\n",
    "checkpoint_name = \"Original_notrain\"\n",
    "\n",
    "#df = pd.read_csv('/kaggle/input/wsdm-preprocessed-full-original/train_preprocessed_FULL_original.csv')\n",
    "#dataframe_path = '/kaggle/input/train-preprocessed-mini-original/train_preprocessed_MINI_original.csv'\n",
    "dataframe_path = config.train_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainning Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T10:58:30.031396Z",
     "iopub.status.busy": "2024-12-31T10:58:30.031115Z",
     "iopub.status.idle": "2024-12-31T10:58:30.090850Z",
     "shell.execute_reply": "2024-12-31T10:58:30.089972Z",
     "shell.execute_reply.started": "2024-12-31T10:58:30.031376Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#training_args = TrainingArguments(\n",
    "#    output_dir=\"output\",\n",
    "#    overwrite_output_dir=True,\n",
    "#    report_to=\"none\",\n",
    "#    num_train_epochs=config.n_epochs,\n",
    "#    per_device_train_batch_size=config.train_batch,\n",
    "#    gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "#    per_device_eval_batch_size=config.eval_batch,\n",
    "#    logging_steps=10,\n",
    "#    eval_strategy=\"epoch\",\n",
    "#    save_strategy=\"steps\",\n",
    "#    save_steps=200,\n",
    "#    optim=config.optim_type,\n",
    "#    fp16=True,\n",
    "#    learning_rate=config.lr,\n",
    "#    warmup_steps=config.warmup_steps,\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T10:58:30.092963Z",
     "iopub.status.busy": "2024-12-31T10:58:30.092618Z",
     "iopub.status.idle": "2024-12-31T10:58:30.097699Z",
     "shell.execute_reply": "2024-12-31T10:58:30.096874Z",
     "shell.execute_reply.started": "2024-12-31T10:58:30.092930Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=config.lora_r,\n",
    "    lora_alpha=config.lora_alpha,\n",
    "    # only target self-attention\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n",
    "    #layers_to_transform=[i for i in range(42) if i >= config.freeze_layers],\n",
    "    lora_dropout=config.lora_dropout,\n",
    "    bias=config.lora_bias,\n",
    "    task_type=TaskType.FEATURE_EXTRACTION, #SEQ_CLS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T10:58:30.098881Z",
     "iopub.status.busy": "2024-12-31T10:58:30.098561Z",
     "iopub.status.idle": "2024-12-31T10:58:30.205889Z",
     "shell.execute_reply": "2024-12-31T10:58:30.204902Z",
     "shell.execute_reply.started": "2024-12-31T10:58:30.098850Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ohmatheus\\AppData\\Local\\Temp\\ipykernel_1200\\552262335.py:1: DtypeWarning: Columns (0,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(dataframe_path)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "      <th>winner</th>\n",
       "      <th>language</th>\n",
       "      <th>class_label</th>\n",
       "      <th>prompt_len</th>\n",
       "      <th>response_a_len</th>\n",
       "      <th>response_b_len</th>\n",
       "      <th>...</th>\n",
       "      <th>prompt_chinese</th>\n",
       "      <th>prompt_round_balance</th>\n",
       "      <th>prompt_curly_balance</th>\n",
       "      <th>prompt_json</th>\n",
       "      <th>prompt_sentiment</th>\n",
       "      <th>response_a_sentiment</th>\n",
       "      <th>response_b_sentiment</th>\n",
       "      <th>cosine_similarity_a</th>\n",
       "      <th>cosine_similarity_b</th>\n",
       "      <th>cosine_similarity_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>53567</td>\n",
       "      <td>What is the difference between marriage licens...</td>\n",
       "      <td>A marriage license is a legal document that al...</td>\n",
       "      <td>A marriage license and a marriage certificate ...</td>\n",
       "      <td>model_b</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>192</td>\n",
       "      <td>3096</td>\n",
       "      <td>3592</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.077778</td>\n",
       "      <td>0.058469</td>\n",
       "      <td>0.139458</td>\n",
       "      <td>0.705321</td>\n",
       "      <td>0.629803</td>\n",
       "      <td>0.075518</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                             prompt  \\\n",
       "0  53567  What is the difference between marriage licens...   \n",
       "\n",
       "                                          response_a  \\\n",
       "0  A marriage license is a legal document that al...   \n",
       "\n",
       "                                          response_b   winner language  \\\n",
       "0  A marriage license and a marriage certificate ...  model_b      NaN   \n",
       "\n",
       "   class_label  prompt_len  response_a_len  response_b_len  ...  \\\n",
       "0            1         192            3096            3592  ...   \n",
       "\n",
       "   prompt_chinese  prompt_round_balance  prompt_curly_balance  prompt_json  \\\n",
       "0             0.0                     0                     0            0   \n",
       "\n",
       "   prompt_sentiment  response_a_sentiment  response_b_sentiment  \\\n",
       "0          0.077778              0.058469              0.139458   \n",
       "\n",
       "   cosine_similarity_a  cosine_similarity_b  cosine_similarity_diff  \n",
       "0             0.705321             0.629803                0.075518  \n",
       "\n",
       "[1 rows x 70 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(dataframe_path)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T10:58:30.212194Z",
     "iopub.status.busy": "2024-12-31T10:58:30.211905Z",
     "iopub.status.idle": "2024-12-31T10:58:30.227854Z",
     "shell.execute_reply": "2024-12-31T10:58:30.227016Z",
     "shell.execute_reply.started": "2024-12-31T10:58:30.212162Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df['prompt'] = df['prompt'].astype(str)\n",
    "df['response_a'] = df['response_a'].astype(str)\n",
    "df['response_b'] = df['response_b'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T10:58:30.228835Z",
     "iopub.status.busy": "2024-12-31T10:58:30.228544Z",
     "iopub.status.idle": "2024-12-31T10:58:31.987413Z",
     "shell.execute_reply": "2024-12-31T10:58:31.986421Z",
     "shell.execute_reply.started": "2024-12-31T10:58:30.228809Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "tokenizer.add_eos_token = True      # We'll add <eos> at the end\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=config.sample_size, random_state=config.random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8765, 70)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T10:58:31.988746Z",
     "iopub.status.busy": "2024-12-31T10:58:31.988410Z",
     "iopub.status.idle": "2024-12-31T10:58:32.000618Z",
     "shell.execute_reply": "2024-12-31T10:58:31.999668Z",
     "shell.execute_reply.started": "2024-12-31T10:58:31.988714Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_train, df_valid = train_test_split(df, test_size=0.1, random_state=config.random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T10:58:32.001612Z",
     "iopub.status.busy": "2024-12-31T10:58:32.001364Z",
     "iopub.status.idle": "2024-12-31T10:58:32.016352Z",
     "shell.execute_reply": "2024-12-31T10:58:32.015349Z",
     "shell.execute_reply.started": "2024-12-31T10:58:32.001593Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Prepare dataset and dataloader\n",
    "dataset_train = Utils.ChatbotArenaDataset(df_train, tokenizer, max_length=2048)\n",
    "dataloader_train = Utils.DataLoader(dataset_train, batch_size=config.train_batch, shuffle=True)\n",
    "\n",
    "dataset_valid = Utils.ChatbotArenaDataset(df_valid, tokenizer, max_length=2048)\n",
    "dataloader_valid = Utils.DataLoader(dataset_valid, batch_size=config.eval_batch, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3944"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataloader_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([     2, 235322,  39038,  ...,      0,      0,      0]),\n",
       " 'attention_mask': tensor([1, 1, 1,  ..., 0, 0, 0]),\n",
       " 'features': tensor([ 1.1900e+02,  9.6200e+02,  4.6900e+02,  2.3000e+01,  2.1800e+02,\n",
       "          1.0600e+02,  1.0000e+00,  1.4000e+01,  4.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  4.0000e+00,  3.0000e+00,  1.0000e+00,  2.2000e+01,\n",
       "          3.4000e+01,  0.0000e+00,  0.0000e+00,  8.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  8.0000e+00,  0.0000e+00,  1.0000e+01,  9.0000e+00,\n",
       "          0.0000e+00,  1.0000e+01,  9.0000e+00,  2.5000e+01,  3.0900e+02,\n",
       "          2.3600e+02,  4.0000e+00,  5.7000e+01,  6.3000e+01,  7.3950e-01,\n",
       "          6.0499e-01,  3.4968e-01,  1.6807e-02,  1.4553e-02,  1.2793e-02,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  1.7500e-01, -1.0284e-01,  1.7500e-01,\n",
       "          9.2266e-01,  9.0321e-01,  1.9445e-02]),\n",
       " 'label': tensor([0.])}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    }
   ],
   "source": [
    "predictionModel = Utils.custom_load_model_chkpt(\n",
    "                        config,\n",
    "                        checkpointName=\"Original_notrain\",\n",
    "                        device=device\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "execution_failed": "2024-12-31T10:59:52.168Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreferencePredictionModel(\n",
       "  (gemma_model): PeftModelForFeatureExtraction(\n",
       "    (base_model): LoraModel(\n",
       "      (model): Gemma2Model(\n",
       "        (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-25): 26 x Gemma2DecoderLayer(\n",
       "            (self_attn): Gemma2Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2304, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2304, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2304, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2304, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2304, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2304, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear4bit(in_features=2048, out_features=2304, bias=False)\n",
       "              (rotary_emb): Gemma2RotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Gemma2MLP(\n",
       "              (gate_proj): Linear4bit(in_features=2304, out_features=9216, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=2304, out_features=9216, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=9216, out_features=2304, bias=False)\n",
       "              (act_fn): PytorchGELUTanh()\n",
       "            )\n",
       "            (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "            (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "            (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "            (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (feature_fc): Linear(in_features=63, out_features=128, bias=True)\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=2432, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Linear(in_features=512, out_features=1, bias=True)\n",
       "    (4): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "execution_failed": "2024-12-31T10:59:52.168Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW([\n",
    "    {'params': predictionModel.gemma_model.parameters(), 'lr': 2e-6},     # Lower learning rate for transformer layers\n",
    "    {'params': predictionModel.classifier.parameters(), 'lr': 1e-3},      # Higher learning rate for custom layers\n",
    "], weight_decay=0.01)\n",
    "\n",
    "#optimizer = optim.AdamW(weight_decay=0.01)\n",
    "#optimizer = optim.Adam(predictionModel.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dataloader_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.7700e+03,  1.5870e+03,  2.4040e+03,  2.1300e+02,  2.0500e+02,\n",
       "          3.4500e+02,  5.6000e+01,  4.4000e+01,  5.6000e+01,  1.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  6.0000e+01,  0.0000e+00,  2.0000e+01,\n",
       "          2.1000e+01,  2.0000e+00,  4.0000e+00,  1.7000e+01,  5.0000e+00,\n",
       "          1.9000e+01,  5.0000e+00,  0.0000e+00,  0.0000e+00,  5.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  4.2000e+01,  2.0000e+00,  1.6000e+01,\n",
       "          4.2000e+01,  2.0000e+00,  1.6000e+01,  5.0800e+02,  2.9600e+02,\n",
       "          5.7400e+02,  1.5000e+01,  1.1000e+01,  6.0000e+00,  6.5480e-01,\n",
       "          6.8683e-02,  1.3977e-01,  2.7119e-02,  2.1424e-02,  2.6206e-02,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  7.0000e+00,\n",
       "          2.0000e+00,  2.0000e+00, -5.0000e-01,  0.0000e+00, -5.3333e-01,\n",
       "          3.1565e-01,  5.6146e-01, -2.4582e-01],\n",
       "        [ 1.3600e+02,  6.4100e+02,  1.4300e+03,  2.1000e+01,  1.1800e+02,\n",
       "          2.4500e+02,  3.0000e+00,  7.0000e+00,  1.9000e+01,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  2.0000e+00,  4.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  2.4000e+01,  1.4100e+02,\n",
       "          3.0100e+02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          5.7237e-01,  5.4682e-01,  2.5544e-02]], device='cuda:0')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats = batch['features'].to(device)\n",
    "feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [1.]], device='cuda:0')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = batch['label'].to(device)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1.], device='cuda:0')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionModel.to(device)\n",
    "predictionModel.eval()\n",
    "with torch.no_grad():\n",
    "    logits = predictionModel(\n",
    "                    input_ids=batch['input_ids'].to(device),\n",
    "                    attention_mask=batch['attention_mask'].to(device),\n",
    "                    features=batch['features'].to(device)\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4935],\n",
       "        [0.4934]], device='cuda:0')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4935, 0.4934], device='cuda:0')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.]], device='cuda:0')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normLogits = (logits>0.5).float()\n",
    "normLogits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = normLogits\n",
    "true_labels = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.]], device='cuda:0')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [1.]], device='cuda:0')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(predictions == true_labels).sum().item() / labels.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7063, device='cuda:0')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = nn.BCELoss()(logits, labels)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2304"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictionModel.gemma_model.config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<bos><prompt>: Что делает этот скрипт?\\n\\nimport datetime\\nimport json\\nfrom time import sleep\\nfrom typing import Optional\\n\\nfrom fastapi import FastAPI, HTTPException, Request, Header\\nfrom fastapi.responses import RedirectResponse\\nfrom deta import Deta\\nimport telebot\\n\\nfrom bot import bot\\nfrom config import config\\n\\nHOST = config.get(\\'host\\', \\'https://0hkqon.deta.dev\\')  # \\'https://0hkqon.deta.dev\\', \\'https://y99nge.deta.dev\\'\\n\\ndeta = Deta()\\ndb = deta.Base(\"stats2\")\\napp = FastAPI()\\n\\n\\n@app.get(\"/\")\\ndef read_root():\\n\\treturn RedirectResponse(\\'/docs\\')\\n\\n\\n@app.get(\"/debug\")\\ndef go_to_visor():\\n\\treturn RedirectResponse(\\'https://web.deta.sh/home/baterflyrity/default/micros/parking_bot_demo\\')\\n\\n\\n@app.get(\\'/hook\\')\\ndef set_webhook():\\n\\tbot.remove_webhook()\\n\\tsleep(0.1)\\n\\tbot.set_webhook(url=f\\'{HOST}/reply\\')\\n\\treturn \\'Webhook was set.\\'\\n\\n\\n@app.get(\\'/unhook\\')\\ndef remove_webhook():\\n\\tbot.remove_webhook()\\n\\treturn \\'Webhook was removed.\\'\\n\\n\\n@app.post(\\'/reply\\')\\nasync def reply_to_message(request: Request, content_type: Optional[str] = Header(\\'\\')):\\n\\tif content_type == \\'application/json\\':\\n\\t\\tjson_string = (await request.body()).decode(\\'utf-8\\')\\n\\t\\ttry:\\n\\t\\t\\tdb.put({\\'date\\': datetime.datetime.now().ctime(), \\'stamp\\': datetime.datetime.now().timestamp(), \\'update\\': json.loads(json_string)})\\n\\t\\texcept Exception as ex:\\n\\t\\t\\tprint(f\\'Can not save message to db: {ex}\\')\\n\\t\\tupdate = telebot.types.Update.de_json(json_string)\\n\\t\\tbot.process_new_updates([update])\\n\\t\\treturn \\'\\'\\n\\telse:\\n\\t\\traise HTTPException(403)\\n\\n\\n@app.get(\\'/stats\\')\\ndef get_stats(last: str = None):\\n\\t# return {item[\\'date\\']: item[\\'update\\'] for item in sorted(list(db.fetch()), key=lambda item: item[\\'stamp\\'], reverse=True)}\\n\\t# return list(db.fetch())\\n\\tresp = db.fetch(limit=1000, last=last)\\n\\treturn {\\'count\\': resp.count, \\'last\\': resp.last, \\'items\\': resp.items}\\n\\n\\n<response_a>: Этот скрипт представляет собой веб-приложение, написанное с использованием фреймворка FastAPI. Вот основные функции, которые выполняет этот скрипт:\\n\\n1. Импортирует необходимые модули и библиотеки, такие как `datetime`, `json`, `telebot` и другие.\\n\\n2. Устанавливает переменную `HOST` на основе конфигурации или использует значение по умолчанию.\\n\\n3. Создает экземпляр базы данных `Deta` и инициализирует приложение FastAPI.\\n\\n4. Определяет маршрут `/` (корневой маршрут), который перенаправляет пользователя на страницу документации `/docs`.\\n\\n5. Определяет маршрут `/debug`, который перенаправляет пользователя на определенный URL-адрес.\\n\\n6. Определяет маршрут `/hook`, который устанавливает вебхук для бота Telegram. Он удаляет предыдущий вебхук, ждет 0,1 секунды и устанавливает новый вебхук с URL-адресом, сформированным на основе `HOST`.\\n\\n7. Определяет маршрут `/unhook`, который удаляет вебхук для бота Telegram.\\n\\n8. Определяет маршрут `/reply`, который обрабатывает входящие сообщения от бота Telegram. Он проверяет тип контента запроса и, если это JSON, сохраняет сообщение в базе данных и обрабатывает обновление с помощью `bot.process_new_updates()`.\\n\\n9. Определяет маршрут `/stats`, который возвращает статистику из базы данных. Он принимает необязательный параметр `last` для пагинации и возвращает количество элементов, последний элемент и сами элементы.\\n\\nВ целом, этот скрипт представляет собой бэкенд-приложение, которое обрабатывает взаимодействие с ботом Telegram, сохраняет данные в базе данных и предоставляет API-эндпоинты для управления вебхуками и получения статистики.\\n\\n<response_b>: Этот скрипт реализует веб-сервис с использованием фреймворка FastAPI и интегрирует его с Telegram ботом через библиотеку `telebot`. Сервис использует базу данных Deta для хранения информации о сообщениях. Давайте рассмотрим основные части скрипта:\\n\\n1. Импорт библиотек и модулей: скрипт импортирует необходимые библиотеки и модули, такие как `datetime`, `json`, `sleep`, `FastAPI`, `HTTPException`, `Request`, `Header`, `RedirectResponse`, `Deta`, `telebot`, а также модули `bot` и `config` (предположительно пользовательские модули).\\n\\n2. Конфигурация:\\n   - `HOST`: URL-адрес сервера, на который будут отправляться обновления от Telegram.\\n   - `deta`: Инициализация клиента для работы с Deta Base, сервисом для хранения данных.\\n   - `db`: Создание или доступ к базе данных с именем \"stats2\" в сервисе Deta Base.\\n   - `app`: Экземпляр FastAPI, который будет обрабатывать HTTP-запросы.\\n\\n3. Маршрутизация и обработчики запросов:\\n   - `@app.get(\"/\")`: Перенаправляет запросы к корневому пути на документацию API (`/docs`).\\n   - `@app.get(\"/debug\")`: Перенаправляет запросы к `/debug` на веб-интерфейс Deta для визуализации и управления приложением.\\n   - `@app.get(\\'/hook\\')`: Устанавливает вебхук для Telegram бота, чтобы получать обновления через заданный URL (`HOST`).\\n   - `@app.get(\\'/unhook\\')`: Удаляет вебхук, отключая получение обновлений от Telegram бота.\\n   - `@app.post(\\'/reply\\')`: Принимает обновления от Telegram (в формате JSON), сохраняет их в базу данных и обрабатывает их с помощью функции `bot.process_new_updates`.\\n   - `@app.get(\\'/stats\\')`: Возвращает статистику из базы данных, включая количество элементов и сами элементы, с возможностью пагинации (через параметр `last`).\\n\\n4. Интеграция с Telegram ботом:\\n   - Установка (`set_webhook`) и удаление (`remove_webhook`) вебхука для получения обновлений от Telegram.\\n   - Обработка входящих сообщений и команд от Telegram в эндпоинте `@app.post(\\'/reply\\')`.\\n\\n5. Работа с базой данных:\\n   - В эндпоинте `@app.post(\\'/reply\\')` информация о полученных обновлениях сохраняется в базе данных Deta с пометками времени.\\n   - В эндпоинте `@app.get(\\'/stats\\')` из базы данных извлекается информация для предоставления статистики.\\n\\nВ целом, этот скрипт предназначен для запуска веб-сервиса, который служит интерфейсом между Telegram ботом и базой данных, позволяя боту обрабатывать сообщения и команды, а также сохранять информацию о произошедших событиях.<eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(batch['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mstop\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2024-12-31T10:59:52.168Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "history = Utils.train_model(predictionModel, dataloader_train, dataloader_valid, optimizer, config, scheduler=None, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Utils.plot_model_history(history, \"Trainning History\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!runpodctl remove pod $RUNPOD_POD_ID"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6401106,
     "sourceId": 10337400,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6401199,
     "sourceId": 10337556,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 215481246,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 205269,
     "modelInstanceId": 183069,
     "sourceId": 214733,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 205295,
     "modelInstanceId": 183093,
     "sourceId": 214762,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 205701,
     "modelInstanceId": 183509,
     "sourceId": 215248,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
