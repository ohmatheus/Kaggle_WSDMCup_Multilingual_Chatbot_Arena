{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T10:58:15.406868Z",
     "iopub.status.busy": "2024-12-31T10:58:15.406529Z",
     "iopub.status.idle": "2024-12-31T10:58:18.880251Z",
     "shell.execute_reply": "2024-12-31T10:58:18.879320Z",
     "shell.execute_reply.started": "2024-12-31T10:58:15.406840Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers: 4.47.1\n"
     ]
    }
   ],
   "source": [
    "# gemma-2 is available from transformers>=4.42.3\n",
    "\n",
    "import transformers as trsf\n",
    "print(\"Transformers:\", trsf.__version__)\n",
    "\n",
    "#!pip install -U \"transformers>=4.42.3\" bitsandbytes accelerate peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T10:58:18.881621Z",
     "iopub.status.busy": "2024-12-31T10:58:18.881321Z",
     "iopub.status.idle": "2024-12-31T10:58:29.909550Z",
     "shell.execute_reply": "2024-12-31T10:58:29.908664Z",
     "shell.execute_reply.started": "2024-12-31T10:58:18.881601Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "from dataclasses import dataclass\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "#from datasets import Dataset\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import (\n",
    "    BitsAndBytesConfig,\n",
    "    Gemma2ForSequenceClassification,\n",
    "    Gemma2Model,\n",
    "    GemmaTokenizerFast,\n",
    "    Gemma2Config,\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    PreTrainedTokenizerBase, \n",
    "    EvalPrediction,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import ModelsUtils as Utils\n",
    "import Configurations as Configs\n",
    "#import wsdm_modelutils as Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peft: 0.14.0\n"
     ]
    }
   ],
   "source": [
    "import peft as pft\n",
    "print(\"Peft:\", pft.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T10:58:29.911929Z",
     "iopub.status.busy": "2024-12-31T10:58:29.911151Z",
     "iopub.status.idle": "2024-12-31T10:58:29.966701Z",
     "shell.execute_reply": "2024-12-31T10:58:29.965793Z",
     "shell.execute_reply.started": "2024-12-31T10:58:29.911901Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.5.1+cu118\n",
      "Torch is build with CUDA: True\n",
      "Torch device : cuda\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('Torch version:', torch.__version__)\n",
    "print('Torch is build with CUDA:', torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Torch device : {device}')\n",
    "print('------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'micro_gemma2_2b_fp16_4bit'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_file = 'Configs.py'\n",
    "manager = Configs.ConfigManager(config_file)\n",
    "\n",
    "config = manager.micro\n",
    "#config = manager.runpod_1\n",
    "#load_from_config = manager.save_load_gemma2_2b_fp16\n",
    "#load_from_config = manager.save_load_gemma2_2b_fp16_hidden_512\n",
    "config.config_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for AutoModel and auto tokenizer\n",
    "#base_model_path = '/kaggle/input/bge-multilingual-gemma2-fp16/pytorch/default/1/bge-multilingual-gemma2-fp16'\n",
    "#base_model_path = '/kaggle/input/bge-multilingual-gemma2-fp16/pytorch/default/1/bge-multilingual-gemma2-fp16-4bit'\n",
    "base_model_path = config.basemodel_path\n",
    "\n",
    "\n",
    "# peft pre saved locally\n",
    "#peft_model_path = '/kaggle/input/peftchkpt_original/pytorch/default/1/'\n",
    "peft_model_path = '../Checkpoints/'\n",
    "checkpoint_name = \"Original_notrain\"\n",
    "\n",
    "#df = pd.read_csv('/kaggle/input/wsdm-preprocessed-full-original/train_preprocessed_FULL_original.csv')\n",
    "#dataframe_path = '/kaggle/input/train-preprocessed-mini-original/train_preprocessed_MINI_original.csv'\n",
    "dataframe_path = config.train_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainning Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T10:58:30.031396Z",
     "iopub.status.busy": "2024-12-31T10:58:30.031115Z",
     "iopub.status.idle": "2024-12-31T10:58:30.090850Z",
     "shell.execute_reply": "2024-12-31T10:58:30.089972Z",
     "shell.execute_reply.started": "2024-12-31T10:58:30.031376Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#training_args = TrainingArguments(\n",
    "#    output_dir=\"output\",\n",
    "#    overwrite_output_dir=True,\n",
    "#    report_to=\"none\",\n",
    "#    num_train_epochs=config.n_epochs,\n",
    "#    per_device_train_batch_size=config.train_batch,\n",
    "#    gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "#    per_device_eval_batch_size=config.eval_batch,\n",
    "#    logging_steps=10,\n",
    "#    eval_strategy=\"epoch\",\n",
    "#    save_strategy=\"steps\",\n",
    "#    save_steps=200,\n",
    "#    optim=config.optim_type,\n",
    "#    fp16=True,\n",
    "#    learning_rate=config.lr,\n",
    "#    warmup_steps=config.warmup_steps,\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T10:58:30.092963Z",
     "iopub.status.busy": "2024-12-31T10:58:30.092618Z",
     "iopub.status.idle": "2024-12-31T10:58:30.097699Z",
     "shell.execute_reply": "2024-12-31T10:58:30.096874Z",
     "shell.execute_reply.started": "2024-12-31T10:58:30.092930Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=config.lora_r,\n",
    "    lora_alpha=config.lora_alpha,\n",
    "    # only target self-attention\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n",
    "    #layers_to_transform=[i for i in range(42) if i >= config.freeze_layers],\n",
    "    lora_dropout=config.lora_dropout,\n",
    "    bias=config.lora_bias,\n",
    "    task_type=TaskType.FEATURE_EXTRACTION, #SEQ_CLS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T10:58:30.098881Z",
     "iopub.status.busy": "2024-12-31T10:58:30.098561Z",
     "iopub.status.idle": "2024-12-31T10:58:30.205889Z",
     "shell.execute_reply": "2024-12-31T10:58:30.204902Z",
     "shell.execute_reply.started": "2024-12-31T10:58:30.098850Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ohmatheus\\AppData\\Local\\Temp\\ipykernel_19752\\552262335.py:1: DtypeWarning: Columns (0,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(dataframe_path)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "      <th>winner</th>\n",
       "      <th>language</th>\n",
       "      <th>class_label</th>\n",
       "      <th>prompt_len</th>\n",
       "      <th>response_a_len</th>\n",
       "      <th>response_b_len</th>\n",
       "      <th>...</th>\n",
       "      <th>prompt_chinese</th>\n",
       "      <th>prompt_round_balance</th>\n",
       "      <th>prompt_curly_balance</th>\n",
       "      <th>prompt_json</th>\n",
       "      <th>prompt_sentiment</th>\n",
       "      <th>response_a_sentiment</th>\n",
       "      <th>response_b_sentiment</th>\n",
       "      <th>cosine_similarity_a</th>\n",
       "      <th>cosine_similarity_b</th>\n",
       "      <th>cosine_similarity_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>53567</td>\n",
       "      <td>What is the difference between marriage licens...</td>\n",
       "      <td>A marriage license is a legal document that al...</td>\n",
       "      <td>A marriage license and a marriage certificate ...</td>\n",
       "      <td>model_b</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>192</td>\n",
       "      <td>3096</td>\n",
       "      <td>3592</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.077778</td>\n",
       "      <td>0.058469</td>\n",
       "      <td>0.139458</td>\n",
       "      <td>0.705321</td>\n",
       "      <td>0.629803</td>\n",
       "      <td>0.075518</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                             prompt  \\\n",
       "0  53567  What is the difference between marriage licens...   \n",
       "\n",
       "                                          response_a  \\\n",
       "0  A marriage license is a legal document that al...   \n",
       "\n",
       "                                          response_b   winner language  \\\n",
       "0  A marriage license and a marriage certificate ...  model_b      NaN   \n",
       "\n",
       "   class_label  prompt_len  response_a_len  response_b_len  ...  \\\n",
       "0            1         192            3096            3592  ...   \n",
       "\n",
       "   prompt_chinese  prompt_round_balance  prompt_curly_balance  prompt_json  \\\n",
       "0             0.0                     0                     0            0   \n",
       "\n",
       "   prompt_sentiment  response_a_sentiment  response_b_sentiment  \\\n",
       "0          0.077778              0.058469              0.139458   \n",
       "\n",
       "   cosine_similarity_a  cosine_similarity_b  cosine_similarity_diff  \n",
       "0             0.705321             0.629803                0.075518  \n",
       "\n",
       "[1 rows x 70 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(dataframe_path)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T10:58:30.212194Z",
     "iopub.status.busy": "2024-12-31T10:58:30.211905Z",
     "iopub.status.idle": "2024-12-31T10:58:30.227854Z",
     "shell.execute_reply": "2024-12-31T10:58:30.227016Z",
     "shell.execute_reply.started": "2024-12-31T10:58:30.212162Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df['prompt'] = df['prompt'].astype(str)\n",
    "df['response_a'] = df['response_a'].astype(str)\n",
    "df['response_b'] = df['response_b'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T10:58:30.228835Z",
     "iopub.status.busy": "2024-12-31T10:58:30.228544Z",
     "iopub.status.idle": "2024-12-31T10:58:31.987413Z",
     "shell.execute_reply": "2024-12-31T10:58:31.986421Z",
     "shell.execute_reply.started": "2024-12-31T10:58:30.228809Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "tokenizer.add_eos_token = True      # We'll add <eos> at the end\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=config.sample_size, random_state=config.random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_df(row):\n",
    "    return Utils.tokenize(tokenizer, [row['prompt']], [row['response_a']], [row['response_b']], max_length=config.max_length)\n",
    "\n",
    "df['tokens'] = df.apply(tokenize_df, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(175, 71)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T10:58:31.988746Z",
     "iopub.status.busy": "2024-12-31T10:58:31.988410Z",
     "iopub.status.idle": "2024-12-31T10:58:32.000618Z",
     "shell.execute_reply": "2024-12-31T10:58:31.999668Z",
     "shell.execute_reply.started": "2024-12-31T10:58:31.988714Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_train, df_valid = train_test_split(df, test_size=0.1, random_state=config.random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T10:58:32.001612Z",
     "iopub.status.busy": "2024-12-31T10:58:32.001364Z",
     "iopub.status.idle": "2024-12-31T10:58:32.016352Z",
     "shell.execute_reply": "2024-12-31T10:58:32.015349Z",
     "shell.execute_reply.started": "2024-12-31T10:58:32.001593Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Prepare dataset and dataloader\n",
    "dataset_train = Utils.ChatbotArenaDataset(df_train, tokenizer, max_length=2048)\n",
    "dataloader_train = Utils.DataLoader(dataset_train, batch_size=config.train_batch, shuffle=True)\n",
    "\n",
    "dataset_valid = Utils.ChatbotArenaDataset(df_valid, tokenizer, max_length=2048)\n",
    "dataloader_valid = Utils.DataLoader(dataset_valid, batch_size=config.eval_batch, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataloader_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    }
   ],
   "source": [
    "predictionModel = Utils.custom_load_model_chkpt(\n",
    "                        config,\n",
    "                        checkpointName=\"Original_notrain\",\n",
    "                        device=device\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "execution_failed": "2024-12-31T10:59:52.168Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreferencePredictionModel(\n",
       "  (gemma_model): PeftModel(\n",
       "    (base_model): LoraModel(\n",
       "      (model): Gemma2Model(\n",
       "        (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-25): 26 x Gemma2DecoderLayer(\n",
       "            (self_attn): Gemma2Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2304, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2304, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2304, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2304, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2304, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2304, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear4bit(in_features=2048, out_features=2304, bias=False)\n",
       "              (rotary_emb): Gemma2RotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Gemma2MLP(\n",
       "              (gate_proj): Linear4bit(in_features=2304, out_features=9216, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=2304, out_features=9216, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=9216, out_features=2304, bias=False)\n",
       "              (act_fn): PytorchGELUTanh()\n",
       "            )\n",
       "            (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "            (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "            (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "            (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (feature_fc): Linear(in_features=63, out_features=128, bias=True)\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=2432, out_features=10, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Linear(in_features=10, out_features=1, bias=True)\n",
       "    (4): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "execution_failed": "2024-12-31T10:59:52.168Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW([\n",
    "    {'params': predictionModel.gemma_model.parameters(), 'lr': 2e-6},     # Lower learning rate for transformer layers\n",
    "    {'params': predictionModel.classifier.parameters(), 'lr': 1e-3},      # Higher learning rate for custom layers\n",
    "], weight_decay=0.01)\n",
    "\n",
    "#optimizer = optim.AdamW(weight_decay=0.01)\n",
    "#optimizer = optim.Adam(predictionModel.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dataloader_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.3400e+02,  2.7320e+03,  9.7900e+02,  6.3000e+01,  5.0000e+02,\n",
       "          2.2000e+02,  8.0000e+00,  4.5000e+01,  1.8000e+01,  1.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00,  5.0000e+00,\n",
       "          0.0000e+00,  3.0000e+00,  0.0000e+00,  5.0000e+00,  1.6000e+01,\n",
       "          9.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          1.0000e+00,  0.0000e+00,  2.0000e+00,  3.0000e+00,  6.0000e+00,\n",
       "          2.0000e+00,  3.0000e+00,  1.2000e+01,  8.3000e+01,  6.1900e+02,\n",
       "          3.0200e+02,  1.1000e+01,  3.9000e+01,  5.3000e+01,  7.0359e-01,\n",
       "          7.3792e-01,  5.9142e-01,  1.4970e-02,  2.0132e-02,  2.3493e-02,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         -6.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00, -3.7500e-02,  5.6452e-02, -1.5357e-01,\n",
       "          5.7135e-01,  7.3935e-01, -1.6800e-01],\n",
       "        [ 1.2300e+02,  2.0190e+03,  5.4190e+03,  2.2000e+01,  3.1700e+02,\n",
       "          8.6700e+02,  0.0000e+00,  3.6000e+01,  1.1800e+02,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  1.7000e+01,  1.5000e+01,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  6.0000e+00,\n",
       "          1.3000e+01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  2.3000e+01,  3.7600e+02,\n",
       "          1.0140e+03,  1.0000e+00,  2.0000e+00,  1.6000e+01,  7.4797e-01,\n",
       "          7.8950e-01,  7.9147e-01,  5.6911e-02,  2.3279e-02,  1.8454e-02,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00, -4.0000e-01,  8.8038e-02,  6.5039e-02,\n",
       "          7.6165e-01,  7.9005e-01, -2.8403e-02]], device='cuda:0')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats = batch['features'].to(device)\n",
    "feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [0.]], device='cuda:0')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = batch['label'].to(device)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0.], device='cuda:0')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The 'batch_size' argument of HybridCache is deprecated and will be removed in v4.49. Use the more precisely named 'max_batch_size' argument instead.\n",
      "The 'batch_size' attribute of HybridCache is deprecated and will be removed in v4.49. Use the more precisely named 'self.max_batch_size' attribute instead.\n"
     ]
    }
   ],
   "source": [
    "predictionModel.to(device)\n",
    "predictionModel.eval()\n",
    "with torch.no_grad():\n",
    "    logits = predictionModel(\n",
    "                    input_ids=batch['input_ids'].to(device),\n",
    "                    attention_mask=batch['attention_mask'].to(device),\n",
    "                    features=batch['features'].to(device)\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4768],\n",
       "        [0.4752]], device='cuda:0')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4768, 0.4752], device='cuda:0')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.]], device='cuda:0')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normLogits = (logits>0.5).float()\n",
    "normLogits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = normLogits\n",
    "true_labels = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.]], device='cuda:0')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [0.]], device='cuda:0')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(predictions == true_labels).sum().item() / labels.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6927, device='cuda:0')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = nn.BCELoss()(logits, labels)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2304"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictionModel.gemma_model.config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<bos><prompt>: A long cylindrical conductor (radius = 1.0 mm) carries a charge density of +8 pC/m and is inside a coaxial, hollow, cylindrical conductor (inner radius = 3.0 mm, outer radius = 5.0 mm) that has a total charge density of -2 pC/m.\\n\\nWhat is the magnitude of the electric field 2.0 mm from the axis of these conductors?\\nAnswer for part 1\\n\\n\\n<response_a>: ## Step 1:  Identify the type of problem and relevant equations.\\nThis problem involves finding the electric field outside a long cylindrical conductor due to a charge distribution. We will use the method of images to solve this. The electric field due to a long cylindrical conductor is given by the Biot-Savart law and is proportional to 1/r. For a coaxial conductor, we also need to consider the field due to the inner conductor.\\n\\n## Step 2:  Determine the charge distribution and the distance to calculate the electric field.\\nThe long cylindrical conductor has a charge density of +8 pC/m, and the coaxial conductor has a total charge density of -2 pC/m. The distance from the axis to<eos>'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(batch['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mstop\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2024-12-31T10:59:52.168Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "history = Utils.train_model(predictionModel, dataloader_train, dataloader_valid, optimizer, config, scheduler=None, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Utils.plot_model_history(history, \"Trainning History\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!runpodctl remove pod $RUNPOD_POD_ID"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6401106,
     "sourceId": 10337400,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6401199,
     "sourceId": 10337556,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 215481246,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 205269,
     "modelInstanceId": 183069,
     "sourceId": 214733,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 205295,
     "modelInstanceId": 183093,
     "sourceId": 214762,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 205701,
     "modelInstanceId": 183509,
     "sourceId": 215248,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
