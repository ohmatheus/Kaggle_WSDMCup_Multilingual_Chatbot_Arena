{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "#from datasets import Dataset\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import (\n",
    "    BitsAndBytesConfig,\n",
    "    Gemma2ForSequenceClassification,\n",
    "    Gemma2Model,\n",
    "    GemmaTokenizerFast,\n",
    "    Gemma2Config,\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    PreTrainedTokenizerBase, \n",
    "    EvalPrediction,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "\n",
    "from peft import LoraModel, PeftModel, LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import ModelsUtils as Utils\n",
    "import Configurations as Configs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.5.1+cu118\n",
      "Torch is build with CUDA: True\n",
      "Torch device : cuda\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('Torch version:', torch.__version__)\n",
    "print('Torch is build with CUDA:', torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Torch device : {device}')\n",
    "print('------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = 'Configs.py'\n",
    "manager = Configs.ConfigManager(config_file)\n",
    "config = manager.micro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    }
   ],
   "source": [
    "predictionModelLoaded = Utils.custom_load_model_chkpt(\n",
    "                        config,\n",
    "                        checkpointName=\"Original_notrain\",\n",
    "                        device=device,\n",
    "                        optimizer=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,761,280 || all params: 2,616,103,168 || trainable%: 0.0673\n"
     ]
    }
   ],
   "source": [
    "predictionModelLoaded.gemma_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreferencePredictionModel(\n",
       "  (gemma_model): PeftModelForFeatureExtraction(\n",
       "    (base_model): LoraModel(\n",
       "      (model): Gemma2ForCausalLM(\n",
       "        (model): Gemma2Model(\n",
       "          (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n",
       "          (layers): ModuleList(\n",
       "            (0-15): 16 x Gemma2DecoderLayer(\n",
       "              (self_attn): Gemma2Attention(\n",
       "                (q_proj): Linear4bit(in_features=2304, out_features=2048, bias=False)\n",
       "                (k_proj): Linear4bit(in_features=2304, out_features=1024, bias=False)\n",
       "                (v_proj): Linear4bit(in_features=2304, out_features=1024, bias=False)\n",
       "                (o_proj): Linear4bit(in_features=2048, out_features=2304, bias=False)\n",
       "                (rotary_emb): Gemma2RotaryEmbedding()\n",
       "              )\n",
       "              (mlp): Gemma2MLP(\n",
       "                (gate_proj): Linear4bit(in_features=2304, out_features=9216, bias=False)\n",
       "                (up_proj): Linear4bit(in_features=2304, out_features=9216, bias=False)\n",
       "                (down_proj): Linear4bit(in_features=9216, out_features=2304, bias=False)\n",
       "                (act_fn): PytorchGELUTanh()\n",
       "              )\n",
       "              (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "              (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "              (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "              (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "            )\n",
       "            (16-25): 10 x Gemma2DecoderLayer(\n",
       "              (self_attn): Gemma2Attention(\n",
       "                (q_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2304, out_features=2048, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2304, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2304, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2304, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (v_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2304, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2304, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): Linear4bit(in_features=2048, out_features=2304, bias=False)\n",
       "                (rotary_emb): Gemma2RotaryEmbedding()\n",
       "              )\n",
       "              (mlp): Gemma2MLP(\n",
       "                (gate_proj): Linear4bit(in_features=2304, out_features=9216, bias=False)\n",
       "                (up_proj): Linear4bit(in_features=2304, out_features=9216, bias=False)\n",
       "                (down_proj): Linear4bit(in_features=9216, out_features=2304, bias=False)\n",
       "                (act_fn): PytorchGELUTanh()\n",
       "              )\n",
       "              (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "              (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "              (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "              (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "            )\n",
       "          )\n",
       "          (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        )\n",
       "        (lm_head): Linear(in_features=2304, out_features=256000, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (feature_fc): Linear(in_features=63, out_features=128, bias=True)\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=2304, out_features=10, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Linear(in_features=10, out_features=1, bias=True)\n",
       "    (4): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictionModelLoaded"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
